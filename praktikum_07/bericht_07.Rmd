---
output: pdf_document
header-includes: 
- \usepackage[utf8]{inputenc}
- \usepackage[T1]{fontenc}
- \usepackage[ngerman]{babel}
- \usepackage{amsmath,amssymb,amsthm}
- \usepackage{dsfont}
- \usepackage{listings}
- \usepackage{enumitem}
- \usepackage{floatrow}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- '\fancyhead[C,C]{Gruppe 5: Robin Baudisch, Merlin Kopfmann, Maximilian Neudert}'
- \fancyhead[L]{}
- \fancyhead[R]{}
- \fancyfoot[C,C]{\thepage}
- \renewcommand{\footrulewidth}{0.4pt}
- \newcommand{\argmin}{\operatorname{arg}\min}
- \newcommand{\R}{\mathds{R}}
---

<style type="text/css">
body{
  font-size: 12px;
}
h1 {
  font-size: 18px;
}
h1 {
  font-size: 14px;
}
h1 {
  font-size: 12px;
}
</style>

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=TRUE,     # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 10,     # set figure width
                      out.width = "100%", # set width of displayed images
                      warning=TRUE,      # show R warnings
                      message=FALSE)     # show R messages
```

<!---** Hochschule Darmstadt | Studiengang Data Science | Sommersemester 2019 **--->

```{r, echo=FALSE}
set.seed(42)
usepackage = function(p) 
{
  if (!is.element(p, installed.packages()[,1]))
    install.packages(p, dep = TRUE)
  require(p, character.only = TRUE)
}
usepackage('ggplot2')
usepackage('MASS')
usepackage('tidyr')
usepackage('boot')
```

# Praktikum 7

## Aufgabe 1

Zuerst berechnen wir die Punkte

```{r}
h1 = function (X) {
  return(X)
}
h2 = function (X) {
  ind = as.numeric(X >= 1)
  return((X-1)^2 * ind)
}
Y = function (X) {
  y = 1 + h1(X) - 2 * h2(X)
  return(y)
}
y1 = Y(-2)
y2 = Y(2)
```

Wir erhalten $Y(-2) = `r y1`$ und $Y(2) = `r y2`$.
Skizzieren ist nun relativ simpel, da die Funktion von $-2$ bis $1$ linear ist mit Punkten $(-2, -1)$ und $(0, 1)$. Danach haben wir eine negative quadratische Funktion mit Punkt $(2,1)$ welche im Punkt $(1,2)$ glatt ist und wir somit nur mit etwas schwung die gerade in eine Kurve skizzieren müssen, die dann in $(2,1)$ endet.
Zum Vergleich ein maschineller Plot:

```{r, echo=FALSE}
x = seq(-2, 2, 0.01)
y = Y(x)
df = data.frame(x = x, y = y)
gg = ggplot(
  data = df,
  mapping = aes(
    x = x,
    y = y
  )
)
gg + geom_smooth(
  method = 'loess',
  formula = 'y~x'
)
```

\cleardoublepage

## Aufgabe 2

Der erste Term

$$
\sum_{i=1}^n (y_i - g(x_i))^2
$$

ist der Least Squares Term. Dieser wird minimal, wenn die Kurven die Trainingspunkte interpolieren. Der zweite Term

$$
\lambda \int_a^b \left\vert g^{(3)} (x) \right\vert^2\ dx
$$

ist ein der Glättungsterm (Misst die ''Kurvigkeit der Funktion'') mit Glättungsparameter $\lambda$, der als Strafterm den Funktionswert für starke Veränderungen erhöht. Die Ableitung einer Funktion ist ein Maß für die ''Veränderung'' einer Funktion. Das heißt, je höher die Ableitung, desto stärker misst man Veränderungen. $\lambda$ ist dann das Gewicht für den Strafterm. 
Erhöht man den Wert des Strafterms für die Veränderung, so steigt der Trainingsfehler (Die Funktion wird glatter und interpoliert weniger). 

$\hat g_2$ lässt kurvigere Funktionen als $\hat g_1$ zu und kann somit besser interpolieren (Kleinerer Trainingsfehler).
Für $\lambda \to \infty$ wird der Strafterm maximiert und nur noch eine maximal glatte Funktion zugelassen, sprich die Zielfunktion strebt gegen eine Gerade.
Für $\lambda \to 0$ wird der Strafterm minimiert, somit ignoriert und folglich strebt die Funktion gegen die Interpolation.

Somit erhalten wir:

\begin{enumerate}[label=\alph*)]
\item $\hat g_1$ hat im Allgemeinen einen kleineren Trainingsfehler, da die Zielfunktion langsamer sich einer Geraden annähert.
\item $\hat g_2$ hat im Allgemeinen einen kleineren Testfehler, da starke Schwankungen schneller glatt gebügelt werden.
\item $\hat g_2$ hat im Allgemeinen einen kleineren Trainingsfehler, da stärkere Interpolation.
\item $\hat g_1$ hat im Allgemeinen einen kleineren Testfehler, da schwächere Interpolation.
\end{enumerate}

\cleardoublepage

## Aufgabe 3

```{r}
df = mcycle
```

### a)

Zuerst schauen wir uns den Plot mit linearer Regression an.

```{r}
gg = ggplot(
  data = df,
  mapping = aes(
    x = times,
    y = accel
  )
)
gg = gg + geom_point()
gg + geom_smooth(
  method = 'lm',
  formula = 'y ~ x'
)
```

Dies sieht nicht so passend aus. Nun betrachten wir die diagnostischen Plots:

```{r, fig.height=9}
lm = lm(accel~times, df)
par(mfrow=c(2,2))
plot(lm)
```

Der Scatterplot lässt vermuten, dass die Datenpunkte nicht linear verteilt sind (Modellverletzung).
Dies erkennt man auch in den diagnostischen Plots, da die Residuen normalverteilt sein sollten, was sie aber nicht sind (Modellverletzung).

### b)

Sei $P_n$ die Menge aller Polynome vom Grad $n$, also:

$$
P_n = \lbrace a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n \rbrace,\quad a_i \in \R
$$

Seien $\hat g_2$ und $\hat g_3$ die Regressionsfunktionen für die polynomiale Regression von Grad $2$ und Grad $3$.
Dann sind unsere Modelle definiert durch:

\begin{align}
& \hat g_2
= \argmin_{g \in P_2} \sum_{i = 1}^n (y_i - g(x_i))^2\\
& \hat g_3
= \argmin_{g \in P_3} \sum_{i = 1}^n (y_i - g(x_i))^2
\end{align}

Da ein Polynom von Grad $n$ genau $n+1$ freie Parameter hat, hat $g_2$ 3 freie Parameter und $g_3$ 4 freie Parameter.

```{r}
# models
p2 = lm(accel~poly(times, 2, raw=TRUE), data=df)
p3 = lm(accel~poly(times, 3, raw=TRUE), data=df)
# manual function for prediction
g2 = function(x) {
  g = p2$coefficients[1]
  g = g + p2$coefficients[2] * x
  g = g + p2$coefficients[3] * x^2
  return(g)
}
g3 = function(x) {
  g = p3$coefficients[1]
  g = g + p3$coefficients[2] * x
  g = g + p3$coefficients[3] * x^2
  g = g + p3$coefficients[4] * x^3
  return(g)
}
# times to check
t = c(10, 20, 30, 40)
# manual prediction
val2 = g2(t)
val3 = g3(t)
# prediction using predict
df.t = data.frame(times = t)
pred2 = predict(p2, df.t)
pred3 = predict(p3, df.t)
# Check: Calculate the difference
d2 = val2 - as.vector(pred2)
d3 = val3 - as.vector(pred3)
# round for visual
val2 = round(val2, 2)
val3 = round(val3, 2)
```

Wir erhalten als Ergebnis

\begin{tabular}{l | c c c c}
$t$ & $10$ & $20$ & $30$ & $40$ \\
\hline
$g_2(t)$ &  $`r val2[1]`$ &  $`r val2[2]`$ &  $`r val2[3]`$ &  $`r val2[4]`$\\
$g_3(t)$ &  $`r val3[1]`$ &  $`r val3[2]`$ &  $`r val3[3]`$ &  $`r val3[4]`$
\end{tabular}

Zur Überprüfung haben wir die Differenzen berechnet und erhalten

\begin{tabular}{l | c c c c}
$t$ & $10$ & $20$ & $30$ & $40$\\
\hline
$g_2(t) - \text{predict}$ &  $`r d2[1]`$ &  $`r d2[2]`$ &  $`r d2[3]`$ &  $`r d2[4]`$\\
$g_3(t) - \text{predict}$ &  $`r d3[1]`$ &  $`r d3[2]`$ &  $`r d3[3]`$ &  $`r d3[4]`$
\end{tabular}

Um zu überprüfen, ob ein lineares Modell ausreichen würde, führen wir einen Anpassungstest auf den Residuen durch. Als Testmethode nehmen wir Shapiro-Wilk.

```{r}
lm = lm(accel~times, data=df)
sw = shapiro.test(lm$residuals)
pval = round(sw$p.value, 5)
sw
```

Wir erhalten einen p-Wert von $`r pval`$ dieser ist deutlich kleiner als `0.05` somit können wir annehmen, dass die Residuen der linearen Regression nicht normalverteilt sind und folgich ist ein lineares Modell nicht ausreichend.

### c)

```{r}
deltas = matrix(ncol = 3, nrow = 10)
for (p in seq(10)) {
  glm =  glm(accel~poly(times, p, raw=TRUE), data=df)
  cv = cv.glm(data = df, glm = glm)
  deltas[p,] = c(p, cv$delta)
  ddf = data.frame(round(deltas, 0))
  names(ddf) = c("degree", "delta_raw", "delta_adj")
}
ddf
```

Wie wir sehen erhalten wir als bestes Modell eine polynomiale Regression von Grad $8$.
Zur veranschaulichung plotten wir dieses:

```{r}
gg = ggplot(
  data = df,
  mapping = aes(
    x = times,
    y = accel
  )
)
gg = gg + geom_point()
gg + geom_smooth(
  method = 'glm',
  formula = y~poly(x, 8, raw=TRUE)
)
```

### d)

Das Polynom ist von Grad $8$ und besitzt somit $9$ freie Parameter.

### e)

```{r}
p2 = glm(accel~poly(times, 2, raw=TRUE), data=df)
p3 = glm(accel~poly(times, 3, raw=TRUE), data=df)
p8 = glm(accel~poly(times, 8, raw=TRUE), data=df)
aic2 = round(AIC(p2), 0)
aic3 = round(AIC(p3), 0)
aic8 = round(AIC(p8), 0)
```

Wir erhalten als AIC Werte für die Modelle:

\begin{tabular}{c | c}
p & AIC\\
\hline
$2$ &  $`r aic2`$\\
$3$ &  $`r aic3`$\\
$8$ &  $`r aic8`$
\end{tabular}

Auch hier würden wir anhand des AIC das Modell mit Grad $8$ also mit $9$ Parametern wählen.

### f)

```{r}
gg = ggplot(
  data = df,
  mapping = aes(
    x = times,
    y = accel
  )
)
gg = gg + geom_point()
gg = gg + geom_smooth(
  method = 'glm',
  formula = y~poly(x, 2, raw=TRUE),
  mapping = aes(
    color = 'Grad 2'
  )
)
gg = gg + geom_smooth(
  method = 'glm',
  formula = y~poly(x, 3, raw=TRUE),
  mapping = aes(
    color = 'Grad 3'
  )
)
gg = gg + geom_smooth(
  method = 'glm',
  formula = y~poly(x, 8, raw=TRUE),
  mapping = aes(
    color = 'Grad 8'
  )
)
gg = gg + scale_colour_manual(name="legend", values=c("green", "blue", "red"))
gg
```

- Grad $2$: Die Anpassung wirkt in gar keinem Bereich wirklich gut.
- Grad $3$: Die Anpassung folgt besser dem verlauf, wirkt aber auch nicht wirklich gut.
- Grad $8$: Von $13$ bis $53$ Millisekunden wirkt die Anpassung sehr gut. An den Rändern stößt am auf größere Abweichungen, die durch den hohen Interpolationsgrad zu erklären sind.

