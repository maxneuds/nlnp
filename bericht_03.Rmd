---
output: pdf_document
header-includes: 
- \usepackage[utf8]{inputenc}
- \usepackage[T1]{fontenc}
- \usepackage[ngerman]{babel}
- \usepackage{amsmath,amssymb,amsthm}
- \usepackage{dsfont}
- \usepackage{listings}
- \usepackage{floatrow}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[C,C]{Robin Baudisch, Merlin Kopfmann, Maximilian Neudert}
- \fancyhead[L]{}
- \fancyhead[R]{}
- \fancyfoot[C,C]{\thepage}
- \renewcommand{\footrulewidth}{0.4pt}
---

<style type="text/css">
body{
  font-size: 12px;
}
h1 {
  font-size: 18px;
}
h1 {
  font-size: 14px;
}
h1 {
  font-size: 12px;
}
</style>

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=TRUE,     # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 10,     # set figure width
                      out.width = "100%", # set width of displayed images
                      warning=TRUE,      # show R warnings
                      message=FALSE)     # show R messages
```

<!---** Hochschule Darmstadt | Studiengang Data Science | Sommersemester 2019 **--->

# Aufgabe 1

## a)

```{r}
set.seed(123)
gendata = function (x, beta, len) {
  return (beta * log(x) + rnorm(len))
}
x <- runif(100,1,5)
y <- gendata(x, 4, 100)
df = data.frame(x,y)
```

## b)

```{r}
lmlinear = lm(y~x, data=df)
fdachlin <- lmlinear$coefficients  %*% c(1,3)
plot(df$x, df$y)
abline(lmlinear, col="red", lwd=2)
```

Hier sehen wir die generierten Punkte und die lineare Regression und wir erhalten $\hat f(3) = 4.01.$

```{r}
lmpoly <- lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5))
fdachpoly <- lmpoly$coefficients %*% c(1,3,3^2,3^3,3^4,3^5)
plot(df$x, df$y)
pts = predict(lmpoly)
points(df$x[order(x)], pts[order(x)], type="l", col="red", lwd=2)
```

Hier sehen wir die generierten Punkte und die polynomiale Regression mit Grad 5 und wir erhalten $\hat f(3) = 4.17.$

## c)

```{r}
y2 <- gendata(3, 4, 1)

quadvolin <- (y2-fdachlin)^2
quadvopol <- (y2-fdachpoly)^2
```

Sei $\Delta = (y - \hat f(x))^2$.
F�r die lineare Regression erhalten wir $\Delta = 1.37$ und f�r die polynomiale Regression erhalten wir $\Delta = 1.03.$

## d)

```{r}
alllin <- matrix(NA,nrow=10000,ncol=6)
colnames(alllin) <- c("fdach","y", "quadVor","EBias", "VarFDach","EQuadVor")
allpol <- matrix(NA,nrow=10000,ncol=6)
colnames(allpol) <- c("fdach","y", "quadVor","EBias", "VarFDach","EQuadVor")
for (i in 1:10000){
  x <- runif(100,1,5)
  residuen <- rnorm(100,0,1)
  b <- 4
  y <- b*log(x)+residuen
  lmlinear <- lm(y~x)
  lmpoly <- lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5))
  alllin[i,1] <- lmlinear$coefficients  %*% c(1,3)
  allpol[i,1] <- lmpoly$coefficients %*% c(1,3,3^2,3^3,3^4,3^5)
  
  alllin[i,2] <- b*log(3)+rnorm(1)
  allpol[i,2] <- alllin[i,2]
  
  alllin[i,3] <- (alllin[i,2]-alllin[i,1])^2
  allpol[i,3] <- (allpol[i,2]-allpol[i,1])^2
}
alllin[,4] <- mean(alllin[,1])-4*log(3)
allpol[,4] <- mean(allpol[,1])-4*log(3)
  
alllin[,5] <- var(alllin[,1])
allpol[,5] <- var(allpol[,1])


x3 <- rep(3,10000)
y3 <- gendata(x3, 4, 10000)
alllin[,6] <- mean((y3-alllin[,1])^2)
allpol[,6] <- mean((y3-allpol[,1])^2)
```

## e)

F�r die lineare Regression erhalten wir:

```{r}
head(alllin[,4:6],1)
```

F�r die polynomiale Regression erhalten wir:

```{r}
head(allpol[,4:6],1)
```


### i)

Der Bias (betragsm��ig) und der erwartete quadratische Vorhersagefehler sind im linearen Modell deutlich gr��er als polynomialen Modell. Die polynomiale Regression hat eine h�here Varianz als die lineare Regression.

### ii)

Eine h�here Varianz f�hrt zu einem niedrigeren Bias (Bias-Varianz-Trade-Off).
Bias und erwartete quadratische Vorhersagefehler gehen Hand in Hand (sind positiv korreliert).

### iii)

```{r}
alllin20 <- matrix(NA,nrow=10000,ncol=6)
colnames(alllin20) <- c("fdach","y", "quadVor","EBias", "VarFDach","EQuadVor")
allpol20 <- matrix(NA,nrow=10000,ncol=6)
colnames(allpol20) <- c("fdach","y", "quadVor","EBias", "VarFDach","EQuadVor")
for (i in 1:10000){
  x <- runif(20,1,5)
  residuen <- rnorm(20,0,1)
  b <- 4
  y <- b*log(x)+residuen
  lmlinear <- lm(y~x)
  lmpoly <- lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5))
  alllin20[i,1] <- lmlinear$coefficients  %*% c(1,3)
  allpol20[i,1] <- lmpoly$coefficients %*% c(1,3,3^2,3^3,3^4,3^5)
  
  alllin20[i,2] <- b*log(3)+rnorm(1)
  allpol20[i,2] <- alllin20[i,2]
  
  alllin20[i,3] <- (alllin20[i,2]-alllin20[i,1])^2
  allpol20[i,3] <- (allpol20[i,2]-allpol20[i,1])^2
}
alllin20[,4] <- mean(alllin20[,1])-4*log(3)
allpol20[,4] <- mean(allpol20[,1])-4*log(3)
  
alllin20[,5] <- var(alllin20[,1])
allpol20[,5] <- var(allpol20[,1])
  
x4 <- rep(3,20)
y4 <- gendata(x4, 4, 20)
alllin20[,6] <- mean((y4-alllin20[,1])^2)
allpol20[,6] <- mean((y4-allpol20[,1])^2)
```

$n = 20$.

```{r}
head(alllin20[,4:6],1)
head(allpol20[,4:6],1)
```

$n = 10000$ zum Vergleich.

```{r}
head(alllin[,4:6],1)
head(allpol[,4:6],1)
```

Wir h�tten erwartet, dass die Prognosewerte f�r die lineare Regression sich nicht gro�artig �ndern. Die f�r die polynomiale Regression hingegen schon. Wir stellen fest, dass der Bias f�r die lineare Regression sich nur kaum �ndert. Die Lineare Regression hat f�r ein kleines $n$ einen besseren erwarteten quadratischen Vorhersagewert.

## f)

Da die Logarithmusfunktions h�chstgradig nicht linear ist, ist die polynomiale Regression ein genaueres Modell und liefert bessere Ergebnisse. Dies wird insbesondere im Bias und im erwarteten quadratischen Vorhersagewert widergespiegelt. Man merkte, dass f�r kleine $n$ die lineare Regression bessere Werte liefert. Dies liegt insbesondere daran, dass bei $n = 20$ der Abstand von Punkten zu einer Geraden sich nicht gro�artig �ndert (generell hoher Bias daf�r niedrige Varianz) aber die polynomiale Regression durch die h�here Varianz bei kleinen Stichproben eher schlechtere Werte liefert, da zwar einige Punkte deutlich besser getroffen werden, daf�r wiederum andere Punkte deutlich weiter verfehlt werden. 